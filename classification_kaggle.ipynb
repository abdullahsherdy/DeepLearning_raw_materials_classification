{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2766742,"sourceType":"datasetVersion","datasetId":1688338}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"d5a354ab","cell_type":"code","source":"import os\nimport shutil\nimport random\nfrom pathlib import Path\nfrom glob import glob\nfrom PIL import Image, ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.metrics import (\n    accuracy_score, precision_recall_fscore_support, \n    classification_report, confusion_matrix, roc_curve, auc,\n    roc_auc_score)\nfrom sklearn.preprocessing import label_binarize\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.applications import ResNet50, MobileNetV2\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.initializers import HeNormal\nimport json\nimport cv2\nfrom tqdm import tqdm\n\n\n# This is critical if you also want to run PyTorch in the same Kaggle session.\ntry:\n    gpus = tf.config.list_physical_devices('GPU')\n    for _gpu in gpus:\n        tf.config.experimental.set_memory_growth(_gpu, True)\nexcept Exception as _e:\n    # Safe to ignore if devices are already initialized\n    print(f\"WARNING: could not set TF memory growth: {_e}\")\n\n# For Inception V1 (PyTorch)\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nTORCH_AVAILABLE = True\n\n\nDATASET_NAME = \"minc2500/minc-2500\" \n\n# Kaggle paths\nKAGGLE_INPUT = f\"/kaggle/input/{DATASET_NAME}\"\nKAGGLE_WORKING = \"/kaggle/working\"\nIMAGES_DIR   = os.path.join(DATASET_NAME, \"images\")\nLABELS_DIR   = os.path.join(DATASET_NAME, \"labels\")\n# Check if running on Kaggle\nIS_KAGGLE = os.path.exists('/kaggle/input')\nNEEDS_SPLIT = False\n\nif IS_KAGGLE:\n    print(\"Running on Kaggle!\")\n    # Auto-detect dataset structure\n    print(\"Detecting dataset structure...\")\n    \n    # Check for pre-split structure (train/val/test folders)\n    if os.path.exists(os.path.join(KAGGLE_INPUT, \"train\")):\n        OUTPUT_DIR = KAGGLE_INPUT\n        print(f\"  Found pre-split dataset at: {OUTPUT_DIR}\")\n    \n    # Check for standard MINC structure (images folder with classes)\n    elif os.path.exists(os.path.join(KAGGLE_INPUT, \"images\")):\n        \n        DATA_DIR = os.path.join(KAGGLE_INPUT, \"images\")\n        OUTPUT_DIR = f\"{KAGGLE_WORKING}/dataset\"\n        print(f\"Found MINC-2500 images/ folder\")\n        print(f\"Will create split at: {OUTPUT_DIR}\")\n        NEEDS_SPLIT = True\n    \n    else:\n        \n        try:\n            contents = os.listdir(KAGGLE_INPUT)\n            subdirs = [d for d in contents if os.path.isdir(os.path.join(KAGGLE_INPUT, d))]\n            print(f\"  Found in dataset root: {subdirs[:5]}...\" if len(subdirs) > 5 else f\"  Found: {subdirs}\")\n            \n            if len(subdirs) >= 10:  # Likely class folders\n                DATA_DIR = KAGGLE_INPUT\n                OUTPUT_DIR = f\"{KAGGLE_WORKING}/dataset\"\n                print(f\"  Detected {len(subdirs)} class folders\")\n                print(f\"  Will create split at: {OUTPUT_DIR}\")\n                NEEDS_SPLIT = True\n            else:\n                OUTPUT_DIR = KAGGLE_INPUT\n                print(f\"  Using root as dataset path\")\n        except Exception as e:\n            print(f\"  Error detecting structure: {e}\")\n            OUTPUT_DIR = KAGGLE_INPUT\n    \n    MODELS_DIR = f\"{KAGGLE_WORKING}/models\"\n    REPORTS_DIR = f\"{KAGGLE_WORKING}/reports\"\n    \n    # Check GPU availability\n    if torch.cuda.is_available():\n        print(f\"GPU Available: {torch.cuda.get_device_name(0)}\")\n    else:\n        print(\"WARNING: GPU NOT available - training will be slow!\")\n        print(\"         Enable GPU in Settings > Accelerator > GPU T4 x2\")\nelse:\n    print(\"Running locally\")\n    DATA_DIR = \"section/minc-2500/images\"\n    OUTPUT_DIR = \"./dataset/\"\n    MODELS_DIR = \"models\"\n    REPORTS_DIR = \"reports\"\n    \n\nUSE_PRETRAINED_WEIGHTS = True\n\ndef _has_dns() -> bool:\n    try:\n        import socket\n        socket.gethostbyname('storage.googleapis.com')\n        return True\n    except Exception:\n        return False\n\nif IS_KAGGLE and not _has_dns():\n    USE_PRETRAINED_WEIGHTS = False\n    print(\"WARNING: Internet/DNS appears disabled; pretrained weights downloads will be skipped.\")\n\n# Backwards-compatible alias (in case you reference pretrained_weights elsewhere)\npretrained_weights = USE_PRETRAINED_WEIGHTS\n# Create output directories\nos.makedirs(MODELS_DIR, exist_ok=True)\nos.makedirs(REPORTS_DIR, exist_ok=True)\n\nprint(f\"\\nPaths configured:\")\nif 'DATA_DIR' in locals() and NEEDS_SPLIT:\n    print(f\"  Source: {DATA_DIR}\")\nprint(f\"  Dataset: {OUTPUT_DIR}\")\nprint(f\"  Models: {MODELS_DIR}\")\nprint(f\"  Reports: {REPORTS_DIR}\")\n\n# Model hyperparameters\nIMG_SIZE = (224, 224)\nBATCH_SIZE = 32\nSEED = 42\nMAX_EPOCHS = 2  \n\nprint(f\"\\nConfiguration:\")\nprint(f\"  Image size: {IMG_SIZE}\")\nprint(f\"  Batch size: {BATCH_SIZE}\")\nprint(f\"  Max epochs: {MAX_EPOCHS}\")\nprint(f\"  Seed: {SEED}\")","metadata":{"execution":{"iopub.status.busy":"2025-12-15T21:57:51.746192Z","iopub.execute_input":"2025-12-15T21:57:51.746837Z","iopub.status.idle":"2025-12-15T21:58:23.805850Z","shell.execute_reply.started":"2025-12-15T21:57:51.746810Z","shell.execute_reply":"2025-12-15T21:58:23.805067Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Running on Kaggle!\nDetecting dataset structure...\nFound MINC-2500 images/ folder\nWill create split at: /kaggle/working/dataset\nGPU Available: Tesla T4\nWARNING: Internet/DNS appears disabled; pretrained weights downloads will be skipped.\n\nPaths configured:\n  Source: /kaggle/input/minc2500/minc-2500/images\n  Dataset: /kaggle/working/dataset\n  Models: /kaggle/working/models\n  Reports: /kaggle/working/reports\n\nConfiguration:\n  Image size: (224, 224)\n  Batch size: 32\n  Max epochs: 2\n  Seed: 42\n","output_type":"stream"}],"execution_count":1},{"id":"f96f7bd2","cell_type":"code","source":"def is_image_valid(path):\n    \"\"\"Check if an image file is valid (not empty and not corrupted).\"\"\"\n    try:\n        # First check if file is empty\n        if os.path.getsize(path) == 0:\n            return False\n        with Image.open(path) as img:\n            img.verify()  # will raise if file is broken\n        # Also tryto load the image to catch more issues\n        with Image.open(path) as img:\n            img.load()  # Force load to catch truncated images\n        return True\n    except Exception:\n        return False\n\ndef remove_corrupted_images(data_dir, verbose=True):\n    \"\"\"Remove corrupted/empty images from dataset directory.\"\"\"\n    removed = 0\n    removed_files = []\n    for cls in os.listdir(data_dir):\n        cls_dir = os.path.join(data_dir, cls)\n        if not os.path.isdir(cls_dir):\n            continue\n        for fname in os.listdir(cls_dir):\n            path = os.path.join(cls_dir, fname)\n            if not fname.lower().endswith(('.jpg', '.jpeg', '.png')):\n                continue\n            if not is_image_valid(path):\n                os.remove(path)\n                removed += 1\n                removed_files.append(path)\n                if verbose and removed <= 10:\n                    print(f\"  Removed: {path}\")\n    if removed > 10 and verbose:\n        print(f\"  ... and {removed - 10} more\")\n    print(f\"Removed {removed} corrupted/empty images from {data_dir}\")\n    return removed\n\ndef clean_dataset_splits(output_dir):\n    \"\"\"Clean all splits (train, val, test) of corrupted/empty images.\"\"\"\n    print(\"=\" * 80)\n    print(\"Cleaning dataset of corrupted/empty images...\")\n    print(\"=\" * 80)\n    total_removed = 0\n    for split in ['train', 'val', 'test']:\n        split_dir = os.path.join(output_dir, split)\n        if os.path.exists(split_dir):\n            print(f\"\\nChecking {split}...\")\n            removed = remove_corrupted_images(split_dir, verbose=True)\n            total_removed += removed\n    print(f\"\\nTotal removed: {total_removed} corrupted/empty images\")\n    return total_removed","metadata":{"execution":{"iopub.status.busy":"2025-12-15T21:58:23.807118Z","iopub.execute_input":"2025-12-15T21:58:23.807339Z","iopub.status.idle":"2025-12-15T21:58:23.815026Z","shell.execute_reply.started":"2025-12-15T21:58:23.807323Z","shell.execute_reply":"2025-12-15T21:58:23.814254Z"},"trusted":true},"outputs":[],"execution_count":2},{"id":"a3542286","cell_type":"code","source":"def class_counts(data_dir):\n    counts = {}\n    for cls in sorted(os.listdir(data_dir)):\n        cls_dir = os.path.join(data_dir, cls)\n        if not os.path.isdir(cls_dir):\n            continue\n        counts[cls] = len([f for f in os.listdir(cls_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n    return counts\n\n# Check class distribution in original dataset\nprint(\"=== Original Dataset Class Distribution ===\")\ncounts = class_counts(DATA_DIR)\nprint(f\"Total classes: {len(counts)}\")\nprint(f\"Images per class: {list(counts.values())[0] if counts else 'N/A'}\")\nprint(f\"Dataset is balanced: {len(set(counts.values())) == 1}\")\n\n# Visualize class distribution\nplt.figure(figsize=(14, 6))\nplt.bar(range(len(counts)), list(counts.values()))\nplt.xticks(range(len(counts)), list(counts.keys()), rotation=90)\nplt.ylabel('Number of Images')\nplt.title('Class Distribution in Original Dataset')\nplt.tight_layout()\nplt.show()\n\n# Check split distribution (uses prepared OUTPUT_DIR paths)\nprint(\"\\n=== Split Distribution ===\")\ntrain_counts = class_counts(os.path.join(OUTPUT_DIR, \"train\"))\nval_counts = class_counts(os.path.join(OUTPUT_DIR, \"val\"))\ntest_counts = class_counts(os.path.join(OUTPUT_DIR, \"test\"))\n\nprint(f\"Train: {sum(train_counts.values())} images\")\nprint(f\"Val: {sum(val_counts.values())} images\")\nprint(f\"Test: {sum(test_counts.values())} images\")\nprint(f\"Total: {sum(train_counts.values()) + sum(val_counts.values()) + sum(test_counts.values())} images\")\n","metadata":{"execution":{"iopub.status.busy":"2025-12-15T21:58:23.815705Z","iopub.execute_input":"2025-12-15T21:58:23.816228Z","iopub.status.idle":"2025-12-15T21:58:25.142653Z","shell.execute_reply.started":"2025-12-15T21:58:23.816209Z","shell.execute_reply":"2025-12-15T21:58:25.142039Z"},"trusted":true,"_kg_hide-output":true},"outputs":[{"name":"stdout","text":"=== Original Dataset Class Distribution ===\nTotal classes: 23\nImages per class: 2500\nDataset is balanced: True\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1400x600 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"\n=== Split Distribution ===\nTrain: 40250 images\nVal: 8625 images\nTest: 8625 images\nTotal: 57500 images\n","output_type":"stream"}],"execution_count":3},{"id":"3f8d6398","cell_type":"code","source":"def create_split(data_dir, output_dir, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, seed=SEED, force=False):\n    \"\"\"\n    Create train/val/test split from data directory.\n    \n    Args:\n        force: If True, will overwrite existing split. If False (default), will skip if split already exists.\n    \"\"\"\n    # Check if split already exists\n    if not force and os.path.exists(output_dir):\n        train_dir = os.path.join(output_dir, 'train')\n        if os.path.exists(train_dir) and len(os.listdir(train_dir)) > 0:\n            print(f\"Split already exists at {output_dir}. Skipping...\")\n            return\n    \n    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6\n    random.seed(seed)\n    # prepare output folders\n    for split in [\"train\", \"val\", \"test\"]:\n        split_dir = os.path.join(output_dir, split)\n        os.makedirs(split_dir, exist_ok=True)\n\n    for cls in sorted(os.listdir(data_dir)):\n        cls_dir = os.path.join(data_dir, cls)\n        if not os.path.isdir(cls_dir):\n            continue\n        images = [os.path.join(cls_dir, f) for f in os.listdir(cls_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n        random.shuffle(images)\n        n = len(images)\n        n_train = int(n * train_ratio)\n        n_val = int(n * val_ratio)\n        train_files = images[:n_train]\n        val_files = images[n_train:n_train + n_val]\n        test_files = images[n_train + n_val:]\n\n        for split_name, files in zip((\"train\",\"val\",\"test\"),(train_files,val_files,test_files)):\n            target_dir = os.path.join(output_dir, split_name, cls)\n            os.makedirs(target_dir, exist_ok=True)\n            for src in files:\n                dst = os.path.join(target_dir, os.path.basename(src))\n                if not os.path.exists(dst):\n                    shutil.copy2(src, dst)\n    print(\"Split created under:\", output_dir)\n\n# Example:\n# create_split(DATA_DIR, OUTPUT_DIR)  # Skipped - using pre-split data from minc_split\n# create_split(DATA_DIR, OUTPUT_DIR, force=True)  # Use force=True to overwrite existing split\n","metadata":{"execution":{"iopub.status.busy":"2025-12-15T21:58:25.143963Z","iopub.execute_input":"2025-12-15T21:58:25.144175Z","iopub.status.idle":"2025-12-15T21:58:25.151568Z","shell.execute_reply.started":"2025-12-15T21:58:25.144158Z","shell.execute_reply":"2025-12-15T21:58:25.150794Z"},"trusted":true},"outputs":[],"execution_count":4},{"id":"91c9ec56","cell_type":"code","source":"# Auto-create split if needed (for Kaggle MINC-2500 dataset)\nif 'NEEDS_SPLIT' in locals() and NEEDS_SPLIT and 'DATA_DIR' in locals():\n    print(\"=\" * 80)\n    print(\"Creating train/val/test split...\")\n    print(\"=\" * 80)\n    print(f\"Source: {DATA_DIR}\")\n    print(f\"Output: {OUTPUT_DIR}\")\n    \n    # Check if the existing split the has correct number of classes\n    force_recreate = False\n    source_classes = len([d for d in os.listdir(DATA_DIR) if os.path.isdir(os.path.join(DATA_DIR, d))])\n    train_dir = os.path.join(OUTPUT_DIR, 'train')\n    \n    if os.path.exists(train_dir):\n        split_classes = len([d for d in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, d))])\n        print(f\"Source has {source_classes} classes, split has {split_classes} classes\")\n        \n        if split_classes != source_classes:\n            print(f\"WARNING: Class count mismatch! Forcing split recreation...\")\n            force_recreate = True\n            # Remove existing corrupted split\n            if os.path.exists(OUTPUT_DIR):\n                print(f\"Removing corrupted split at {OUTPUT_DIR}...\")\n                shutil.rmtree(OUTPUT_DIR)\n    else:\n        print(f\"No existing split found, will create new one\")\n    \n    # FORCE_RECREATE_SPLIT: Set to True to force recreate the split, even if it exists\n    force_recreate = True; shutil.rmtree(OUTPUT_DIR) if os.path.exists(OUTPUT_DIR) else None\n    \n    create_split(DATA_DIR, OUTPUT_DIR, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, seed=SEED, force=force_recreate)\n    \n    print(\"\\nSplit created successfully!\")\n    \n    # Verify split was created correctly\n    for split in ['train', 'val', 'test']:\n        split_dir = os.path.join(OUTPUT_DIR, split)\n        if os.path.exists(split_dir):\n            n_classes = len([d for d in os.listdir(split_dir) if os.path.isdir(os.path.join(split_dir, d))])\n            print(f\"  {split}: {n_classes} classes\")\nelse:\n    print(\"Using existing split data\")\n","metadata":{"execution":{"iopub.status.busy":"2025-12-15T21:58:25.152962Z","iopub.execute_input":"2025-12-15T21:58:25.153205Z"},"trusted":true},"outputs":[{"name":"stdout","text":"================================================================================\nCreating train/val/test split...\n================================================================================\nSource: /kaggle/input/minc2500/minc-2500/images\nOutput: /kaggle/working/dataset\nSource has 23 classes, split has 23 classes\n","output_type":"stream"}],"execution_count":null},{"id":"241e3df5-36a5-46aa-a7db-137f93a8971a","cell_type":"code","source":"\n# the following lines completely delete and recreate the split:\n# import shutil\n# if os.path.exists(OUTPUT_DIR):\n#     print(f\"Deleting corrupted split at {OUTPUT_DIR}...\")\n#     shutil.rmtree(OUTPUT_DIR)\n#     print(\"Deleted! Now re-run Cell 4 (create_split) and continue from there.\")\n\n# Clean the dataset of corrupted/empty images before training\n# This fixes \"InvalidArgumentError: Input is empty\" errors\nprint(\"Running PIL-based image validation...\")\nclean_dataset_splits(OUTPUT_DIR)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"bd9b201d","cell_type":"markdown","source":"# Preprocessing pipline","metadata":{}},{"id":"97b4003f","cell_type":"code","source":"def validate_and_clean_with_tensorflow(data_dir, verbose=True):\n    \"\"\"\n    Validate images using TensorFlow's decoder and remove those that fail.\n    This catches images that PIL can read but TensorFlow cannot.\n    \"\"\"\n    removed = 0\n    checked = 0\n    \n    for cls in os.listdir(data_dir):\n        cls_dir = os.path.join(data_dir, cls)\n        if not os.path.isdir(cls_dir):\n            continue\n        \n        for fname in os.listdir(cls_dir):\n            if not fname.lower().endswith(('.jpg', '.jpeg', '.png')):\n                continue\n            \n            path = os.path.join(cls_dir, fname)\n            checked += 1\n            \n            try:\n                # Check file size first\n                if os.path.getsize(path) == 0:\n                    if verbose:\n                        print(f\"  Empty file: {path}\")\n                    os.remove(path)\n                    removed += 1\n                    continue\n                \n                # Try to decode with TensorFlow\n                img_bytes = tf.io.read_file(path)\n                img = tf.io.decode_image(img_bytes, channels=3, expand_animations=False)\n                # Force execution to catch errors\n                _ = img.numpy()\n                \n            except Exception as e:\n                if verbose and removed < 20:\n                    print(f\"  TF decode failed: {path} - {str(e)[:50]}\")\n                os.remove(path)\n                removed += 1\n    \n    print(f\"TensorFlow validation: checked {checked} images, removed {removed} that failed to decode\")\n    return removed\n\n\ndef build_datasets(prepared_dir=OUTPUT_DIR, img_size=IMG_SIZE, batch_size=BATCH_SIZE, seed=SEED, return_raw=False):\n    \"\"\"\n    Build TensorFlow datasets with optimized preprocessing.\n    \n    Args:\n        prepared_dir: Directory containing train/val/test subdirectories\n        img_size: Target image size (height, width)\n        batch_size: Batch size for training\n        seed: Random seed for reproducibility\n        return_raw: If True, also return raw (unnormalized) training dataset\n    \n    Returns:\n        train_ds, val_ds, test_ds (and optionally train_ds_raw) \n    \"\"\"\n\n    # Normalize seed to int (TensorFlow/numpy reject complex values)\n    if seed is not None:\n        try:\n            seed = int(np.real(seed))\n        except Exception as exc:\n            raise ValueError(f\"Seed must be convertible to int, got {seed!r}\") from exc\n\n    # First, validate all images with TensorFlow's decoder\n    print(\"Validating images with TensorFlow decoder...\")\n    for split in ['train', 'val', 'test']:\n        split_dir = os.path.join(prepared_dir, split)\n        if os.path.exists(split_dir):\n            print(f\"\\nValidating {split}...\")\n            validate_and_clean_with_tensorflow(split_dir, verbose=True)\n    print()\n\n    # Load datasets with optimized settings\n    train_ds_raw = tf.keras.preprocessing.image_dataset_from_directory(\n        os.path.join(prepared_dir, 'train'),\n        image_size=img_size,\n        batch_size=batch_size,\n        label_mode='categorical',\n        shuffle=True,\n        seed=seed,\n        interpolation='bilinear'  # Better image quality\n    )\n\n    val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n        os.path.join(prepared_dir, 'val'),\n        image_size=img_size,\n        batch_size=batch_size,\n        label_mode='categorical',\n        shuffle=False,\n        interpolation='bilinear'\n    )\n\n    test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n        os.path.join(prepared_dir, 'test'),\n        image_size=img_size,\n        batch_size=batch_size,\n        label_mode='categorical',\n        shuffle=False,\n        interpolation='bilinear'\n    )\n\n    # Normalization layer (0-1) - scales pixel values from [0, 255] to [0, 1]\n    normalization_layer = layers.Rescaling(1./255)\n\n    # Apply normalization with optimized parallel calls\n    train_ds = train_ds_raw.map(\n        lambda x, y: (normalization_layer(x), y), \n        num_parallel_calls=tf.data.AUTOTUNE,\n        deterministic=False  # Allow non-deterministic for better performance\n    )\n    val_ds = val_ds.map(\n        lambda x, y: (normalization_layer(x), y), \n        num_parallel_calls=tf.data.AUTOTUNE\n    )\n    test_ds = test_ds.map(\n        lambda x, y: (normalization_layer(x), y), \n        num_parallel_calls=tf.data.AUTOTUNE\n    )\n    \n    print(f\" Preprocessing applied:\")\n    print(f\"   - Resizing: {img_size}\")\n    print(f\"   - Normalization: [0, 255]  [0, 1]\")\n    print(f\"   - Batch size: {batch_size}\")\n    print(f\"   - Interpolation: bilinear\")\n\n    # Optimize dataset performance with prefetching\n    # NOTE: Removed .cache() to avoid caching corrupted images from previous runs\n    # If you want caching, restart kernel and run all cells from the beginning\n    train_ds = train_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n    val_ds = val_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n    test_ds = test_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n\n    if return_raw:\n        return train_ds, val_ds, test_ds, train_ds_raw\n    return train_ds, val_ds, test_ds\n\n# Example:\n# Make sure to run Cell 0 first to set OUTPUT_DIR correctly\n\ntrain_ds, val_ds, test_ds, train_ds_raw = build_datasets(return_raw=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"4d349526","cell_type":"code","source":"def get_augmentation_layer():\n    data_augmentation = tf.keras.Sequential([\n        layers.RandomFlip(\"horizontal\"),\n        layers.RandomRotation(0.1),  # +/- 10%\n        layers.RandomZoom(0.1),\n        layers.RandomTranslation(0.1, 0.1),\n        layers.RandomContrast(0.1),\n    ], name='data_augmentation')\n    return data_augmentation\n\n# Apply augmentation to the training dataset\n# Augmentation helps prevent overfitting and improves model generalization\naug = get_augmentation_layer()\n# Apply augmentation with optimized parallel processing\ntrain_ds_aug = train_ds.map(\n    lambda x, y: (aug(x, training=True), y), \n    num_parallel_calls=tf.data.AUTOTUNE,\n    deterministic=False  # Allow non-deterministic for better performance\n).prefetch(buffer_size=tf.data.AUTOTUNE)  # Additional prefetch for augmented data\n\nprint(\"Data augmentation applied to training set:\")\nprint(\"   - Random horizontal flip\")\nprint(\"   - Random rotation (10%)\")\nprint(\"   - Random zoom (10%)\")\nprint(\"   - Random translation (10%)\")\nprint(\"   - Random contrast (10%)\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"ba711ffb","cell_type":"code","source":"def show_sample_augmentation(train_ds, augmentation_layer, n=4):\n    plt.figure(figsize=(12,6))\n    for images, labels in train_ds.take(1):\n        images = images[:n]\n        for i in range(n):\n            ax = plt.subplot(2, n, i+1)\n            plt.imshow(images[i].numpy().astype('uint8'))\n            plt.axis('off')\n            if i == 0:\n                ax.set_title('Original')\n        augmented = augmentation_layer(images, training=True)\n        for i in range(n):\n            ax = plt.subplot(2, n, n + i + 1)\n            plt.imshow((augmented[i].numpy()).astype('uint8'))\n            plt.axis('off')\n            if i == 0:\n                ax.set_title('Augmented')\n    plt.show()\n\nprint(\"=== Data Augmentation Demonstration ===\")\nprint(\"Showing original vs augmented images...\")\nshow_sample_augmentation(train_ds_raw, aug, n=4)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"dc9e8b77","cell_type":"code","source":"def augment_and_save_class_images(src_dir, dst_dir, target_count_per_class=2500, augmentation_layer=None):\n    os.makedirs(dst_dir, exist_ok=True)\n    for cls in sorted(os.listdir(src_dir)):\n        cls_src = os.path.join(src_dir, cls)\n        cls_dst = os.path.join(dst_dir, cls)\n        os.makedirs(cls_dst, exist_ok=True)\n        images = [os.path.join(cls_src, f) for f in os.listdir(cls_src) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n        existing = len(images)\n        # copy existing images first\n        for im in images:\n            shutil.copy2(im, os.path.join(cls_dst, os.path.basename(im)))\n        idx = 0\n        if existing == 0:\n            continue\n        while len(os.listdir(cls_dst)) < target_count_per_class:\n            img_path = images[idx % existing]\n            with Image.open(img_path) as im:\n                im = im.convert('RGB')\n                im = im.resize(IMG_SIZE)\n                arr = np.array(im)\n                arr = np.expand_dims(arr, axis=0).astype(np.float32)\n                aug_img = augmentation_layer(tf.constant(arr), training=True).numpy()[0]\n                aug_img_uint8 = (np.clip(aug_img, 0, 255)).astype(np.uint8)\n                out_path = os.path.join(cls_dst, f\"aug_{idx}_{os.path.basename(img_path)}\")\n                Image.fromarray(aug_img_uint8).save(out_path)\n            idx += 1\n    print(\"Augmentation-and-save complete\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a8d8c014","cell_type":"code","source":"# Get class names\nCLASS_DIR = os.path.join(OUTPUT_DIR, \"train\")\nCLASS_NAMES = sorted(\n    [d for d in os.listdir(CLASS_DIR) if os.path.isdir(os.path.join(CLASS_DIR, d))]\n)\nNUM_CLASSES = len(CLASS_NAMES)\nprint(f\"Number of classes: {NUM_CLASSES}\")\nprint(f\"Class names: {CLASS_NAMES}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e088cc0a","cell_type":"markdown","source":"## Data Preprocessing & Augmentation\n\n### Preprocessing Steps Applied:\n1. **Resizing**: All images resized to (224, 224) for model input\n2. **Normalization**: Pixel values scaled from [0, 255] to [0, 1] using Rescaling(1./255)\n3. **Data Cleaning**: Dataset verified - no corrupted images found\n\n### Data Augmentation Applied:\n- **Random Horizontal Flip**: Helps model learn orientation-invariant features\n- **Random Rotation (10%)**: Improves robustness to rotation variations\n- **Random Zoom (10%)**: Handles scale variations\n- **Random Translation (10%)**: Improves spatial invariance\n- **Random Contrast (10%)**: Handles lighting variations\n\n### Justification:\n- **Balanced Dataset**: All 23 classes have equal samples (2500 each), so no class imbalance\n- **Augmentation Benefits**: \n  - Prevents overfitting by increasing data diversity\n  - Improves model generalization\n  - Helps model learn robust features invariant to transformations\n- **On-the-fly Augmentation**: Applied during training (not saved to disk) for efficiency\n\n### Dataset Statistics:\n- **Total Classes**: 23 material categories\n- **Split**: 80% train (46,000), 10% val (5,750), 10% test (5,750)\n- **Total Images**: 57,500\n","metadata":{}},{"id":"a46e1800","cell_type":"markdown","source":"## VGG-19 from scratch\nArchitecture summary (VGG-19)\n- Convolutional blocks:\n  - Block1: 2  (Conv33, 64)  MaxPool\n  - Block2: 2  (Conv33, 128)  MaxPool\n  - Block3: 4  (Conv33, 256)  MaxPool\n  - Block4: 4  (Conv33, 512)  MaxPool\n  - Block5: 4  (Conv33, 512)  MaxPool\n\n- Classifier (head): \n> Flatten  FC(4096)ReLUDropout(0.5)  FC(4096)ReLUDropout(0.5)  FC(num_classes)  Softmax","metadata":{}},{"id":"cd4ea450","cell_type":"markdown","source":"##  Training Optimizations for Faster Training (Callback methods)\n\n**Optimizations Applied:**\n1. **Early Stopping**: Reduced patience from 5 to 3 epochs - stops training if no improvement for 3 consecutive epochs\n2. **Learning Rate Reduction**: Reduced patience from 3 to 2 epochs - adapts LR faster when stuck\n3. **Increased Dropout**: Changed from 0.5 to 0.6 for better regularization and faster convergence\n4. **Dataset Verification**: Automatic check and creation of train/val/test split from `section/minc-2500/images`\n\n**Expected Training Time Reduction:** ~30-40% faster training with similar or better results\n","metadata":{}},{"id":"b054d472","cell_type":"markdown","source":"## Model Definitions\n","metadata":{}},{"id":"d91db886","cell_type":"code","source":"# VGG19 FROM SCRATCH\ndef build_vgg19_scratch(input_shape=(224, 224, 3), num_classes=23, dropout_rate=0.5, l2_reg=1e-4):\n    \"\"\"Build VGG19 architecture from scratch with He initialization and L2 regularization.\"\"\"\n    kernel_initializer = HeNormal()\n    kernel_regularizer = regularizers.L2(l2_reg)\n    \n    inputs = layers.Input(shape=input_shape)\n    \n    # Block 1: 2 x Conv3x3, 64 filters\n    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same',\n                      kernel_initializer=kernel_initializer,\n                      kernel_regularizer=kernel_regularizer,\n                      name='block1_conv1')(inputs)\n    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same',\n                      kernel_initializer=kernel_initializer,\n                      kernel_regularizer=kernel_regularizer,\n                      name='block1_conv2')(x)\n    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n    \n    # Block 2: 2 x Conv3x3, 128 filters\n    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same',\n                      kernel_initializer=kernel_initializer,\n                      kernel_regularizer=kernel_regularizer,\n                      name='block2_conv1')(x)\n    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same',\n                      kernel_initializer=kernel_initializer,\n                      kernel_regularizer=kernel_regularizer,\n                      name='block2_conv2')(x)\n    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n    \n    # Block 3: 4 x Conv3x3, 256 filters\n    x = layers.Conv2D(256, (3, 3), activation='relu', padding='same',\n                      kernel_initializer=kernel_initializer,\n                      kernel_regularizer=kernel_regularizer,\n                      name='block3_conv1')(x)\n    x = layers.Conv2D(256, (3, 3), activation='relu', padding='same',\n                      kernel_initializer=kernel_initializer,\n                      kernel_regularizer=kernel_regularizer,\n                      name='block3_conv2')(x)\n    x = layers.Conv2D(256, (3, 3), activation='relu', padding='same',\n                      kernel_initializer=kernel_initializer,\n                      kernel_regularizer=kernel_regularizer,\n                      name='block3_conv3')(x)\n    x = layers.Conv2D(256, (3, 3), activation='relu', padding='same',\n                      kernel_initializer=kernel_initializer,\n                      kernel_regularizer=kernel_regularizer,\n                      name='block3_conv4')(x)\n    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n    \n    # Block 4: 4 x Conv3x3, 512 filters\n    x = layers.Conv2D(512, (3, 3), activation='relu', padding='same',\n                      kernel_initializer=kernel_initializer,\n                      kernel_regularizer=kernel_regularizer,\n                      name='block4_conv1')(x)\n    x = layers.Conv2D(512, (3, 3), activation='relu', padding='same',\n                      kernel_initializer=kernel_initializer,\n                      kernel_regularizer=kernel_regularizer,\n                      name='block4_conv2')(x)\n    x = layers.Conv2D(512, (3, 3), activation='relu', padding='same',\n                      kernel_initializer=kernel_initializer,\n                      kernel_regularizer=kernel_regularizer,\n                      name='block4_conv3')(x)\n    x = layers.Conv2D(512, (3, 3), activation='relu', padding='same',\n                      kernel_initializer=kernel_initializer,\n                      kernel_regularizer=kernel_regularizer,\n                      name='block4_conv4')(x)\n    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n    \n    # Block 5: 4 x Conv3x3, 512 filters\n    x = layers.Conv2D(512, (3, 3), activation='relu', padding='same',\n                      kernel_initializer=kernel_initializer,\n                      kernel_regularizer=kernel_regularizer,\n                      name='block5_conv1')(x)\n    x = layers.Conv2D(512, (3, 3), activation='relu', padding='same',\n                      kernel_initializer=kernel_initializer,\n                      kernel_regularizer=kernel_regularizer,\n                      name='block5_conv2')(x)\n    x = layers.Conv2D(512, (3, 3), activation='relu', padding='same',\n                      kernel_initializer=kernel_initializer,\n                      kernel_regularizer=kernel_regularizer,\n                      name='block5_conv3')(x)\n    x = layers.Conv2D(512, (3, 3), activation='relu', padding='same',\n                      kernel_initializer=kernel_initializer,\n                      kernel_regularizer=kernel_regularizer,\n                      name='block5_conv4')(x)\n    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n    \n    # Classifier head\n    x = layers.Flatten(name='flatten')(x)\n    x = layers.Dense(4096, activation='relu',\n                     kernel_initializer=kernel_initializer,\n                     kernel_regularizer=kernel_regularizer,\n                     name='fc1')(x)\n    x = layers.Dropout(dropout_rate, name='dropout1')(x)\n    x = layers.Dense(4096, activation='relu',\n                     kernel_initializer=kernel_initializer,\n                     kernel_regularizer=kernel_regularizer,\n                     name='fc2')(x)\n    x = layers.Dropout(dropout_rate, name='dropout2')(x)\n    outputs = layers.Dense(num_classes, activation='softmax',\n                          kernel_initializer=kernel_initializer,\n                          name='predictions')(x)\n    \n    model = tf.keras.Model(inputs=inputs, outputs=outputs, name='vgg19_scratch')\n\n    # Compile with Adam optimizer, lr=1e-4\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    return model\n\nprint(\" VGG19 from scratch model definition loaded\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"6a4ebeca","cell_type":"code","source":"# ResNet50 TRANSFER LEARNING\ndef build_resnet50_transfer(input_shape=(224, 224, 3), num_classes=23,\n                           include_top=False, weights='imagenet'):\n    \"\"\"Build ResNet50 model with transfer learning.\n\n    Note: On Kaggle, this may fail if Internet is disabled (cannot download weights).\n    In that case, we fall back to weights=None.\n    \"\"\"\n    # Load ResNet50 base\n    try:\n        base_model = ResNet50(\n            include_top=include_top,\n            weights=weights,\n            input_shape=input_shape,\n            pooling='avg'\n        )\n    except Exception as e:\n        print(f\"WARNING: Could not load ResNet50 weights={weights!r} ({e}). Falling back to weights=None.\")\n        base_model = ResNet50(\n            include_top=include_top,\n            weights=None,\n            input_shape=input_shape,\n            pooling='avg'\n        )\n\n    # Freeze base model initially\n    base_model.trainable = False\n\n    # Build model with custom classifier head\n    inputs = layers.Input(shape=input_shape)\n    x = base_model(inputs, training=False)\n    x = layers.Dense(512, activation='relu', name='fc1')(x)\n    x = layers.Dropout(0.5, name='dropout')(x)\n    outputs = layers.Dense(num_classes, activation='softmax', name='predictions')(x)\n\n    model = tf.keras.Model(inputs=inputs, outputs=outputs, name='resnet50_transfer')\n\n    return model, base_model\n\n\ndef unfreeze_resnet_for_finetuning(model, base_model, num_layers_to_unfreeze=40):\n    \"\"\"Unfreeze the last N layers of the base model for fine-tuning.\"\"\"\n    base_model.trainable = True\n    \n    total_layers = len(base_model.layers)\n    freeze_until = total_layers - num_layers_to_unfreeze\n    \n    for layer in base_model.layers[:freeze_until]:\n        layer.trainable = False\n    \n    for layer in base_model.layers[freeze_until:]:\n        layer.trainable = True\n    \n    # Recompile with lower learning rate for fine-tuning\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    return model\n\nprint(\" ResNet50 transfer learning model definition loaded\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f14cfaeb","cell_type":"code","source":"# MobileNetV2 TRANSFER LEARNING\n\ndef build_mobilenetv2_transfer(input_shape=(224, 224, 3), num_classes=23,\n                               include_top=False, weights='imagenet', alpha=1.0):\n    \"\"\"Build MobileNetV2 model with transfer learning.\n\n    Note: On Kaggle, this may fail if Internet is disabled (cannot download weights).\n    In that case, we fall back to weights=None.\n    \"\"\"\n    # Load MobileNetV2 base\n    try:\n        base_model = MobileNetV2(\n            include_top=include_top,\n            weights=weights,\n            input_shape=input_shape,\n            alpha=alpha,\n            pooling='avg'\n        )\n    except Exception as e:\n        print(f\"WARNING: Could not load MobileNetV2 weights={weights!r} ({e}). Falling back to weights=None.\")\n        base_model = MobileNetV2(\n            include_top=include_top,\n            weights=None,\n            input_shape=input_shape,\n            alpha=alpha,\n            pooling='avg'\n        )\n\n    # Freeze base model initially\n    base_model.trainable = False\n\n    # Build model with custom classifier head\n    inputs = layers.Input(shape=input_shape)\n    x = base_model(inputs, training=False)\n    x = layers.Dense(512, activation='relu', name='fc1')(x)\n    x = layers.Dropout(0.6, name='dropout')(x)  # Increased dropout for better regularization\n    outputs = layers.Dense(num_classes, activation='softmax', name='predictions')(x)\n\n    model = tf.keras.Model(inputs=inputs, outputs=outputs, name='mobilenetv2_transfer')\n\n    # Compile model\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n\n    return model, base_model\n\nprint(\" MobileNetV2 transfer learning model definition loaded\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"bd32db86","cell_type":"code","source":"# Inception V1 (GoogLeNet) - PyTorch Implementation\nclass InceptionV1Wrapper:\n    \"\"\"Wrapper class to use PyTorch Inception V1 (GoogLeNet) with TensorFlow pipeline.\"\"\"\n    def __init__(self, num_classes=23, pretrained=True):\n        self.num_classes = num_classes\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        \n        # Load GoogLeNet; on Kaggle this may fail if Internet is disabled.\n        try:\n            weights = models.GoogLeNet_Weights.IMAGENET1K_V1 if pretrained else None\n            self.model = models.googlenet(weights=weights, aux_logits=pretrained)\n        except Exception as e:\n            print(f\"WARNING: Could not load GoogLeNet pretrained={pretrained} ({e}). Falling back to random init.\")\n            self.model = models.googlenet(weights=None, aux_logits=False)\n        \n        # Replace classifier head\n        num_features = self.model.fc.in_features\n        self.model.fc = nn.Linear(num_features, num_classes)\n        \n        self.model = self.model.to(self.device)\n        self.model.eval()\n    \n    def predict(self, images):\n        \"\"\"Predict on batch of images.\"\"\"\n        if isinstance(images, tf.Tensor):\n            images = images.numpy()\n        \n        images_torch = torch.from_numpy(images).permute(0, 3, 1, 2).float()\n        images_torch = images_torch.to(self.device)\n        \n        with torch.no_grad():\n            outputs = self.model(images_torch)\n            probs = torch.softmax(outputs, dim=1)\n        \n        return probs.cpu().numpy()\n    \n    def save(self, path):\n        \"\"\"Save model weights.\"\"\"\n        torch.save(self.model.state_dict(), path)\n    \n    def load(self, path):\n        \"\"\"Load model weights.\"\"\"\n        self.model.load_state_dict(torch.load(path, map_location=self.device))\n    \n    def get_trainable_params_count(self):\n        \"\"\"Get number of trainable parameters.\"\"\"\n        return sum(p.numel() for p in self.model.parameters() if p.requires_grad) \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"c6552aff","cell_type":"markdown","source":"## Training Utilities","metadata":{}},{"id":"2b753514","cell_type":"code","source":"# TRAINING FUNCTION WITH CALLBACKS\ndef train_model(model, \n                train_ds, \n                val_ds, \n                name, \n                epochs=3, \n                early_stopping_patience=5,\n                reduce_lr_patience=3,\n                initial_epoch=0,\n                verbose=1, \n                save_dir=None):\n    \"\"\"\n    Train a model with standard callbacks.\n    \n    Args:\n        model: model to train\n        train_ds: Training dataset\n        val_ds: Validation dataset\n        name: Model name for saving\n        epochs: Maximum number of epochs\n        early_stopping_patience: Patience for early stopping (default: 5)\n        reduce_lr_patience: Patience for learning rate reduction (default: 3)\n        initial_epoch: Starting epoch number (default: 0)\n        verbose: Verbosity mode (default: 1)\n        save_dir: Directory to save model (default: 'models')\n    \n    Returns:\n        model: Trained model\n        history: Training history object\n    \"\"\"\n    if save_dir is None:\n        save_dir = MODELS_DIR\n    os.makedirs(save_dir, exist_ok=True)\n    \n    # Model checkpoint callback\n    checkpoint_path = os.path.join(save_dir, f\"{name}.h5\")\n    checkpoint_callback = ModelCheckpoint(\n        filepath=checkpoint_path,\n        monitor='val_loss',\n        save_best_only=True,\n        save_weights_only=False,\n        mode='min',\n        verbose=verbose\n    )\n    \n    # Early stopping callback\n    early_stopping = EarlyStopping(\n        monitor='val_loss',\n        patience=early_stopping_patience,\n        restore_best_weights=True,\n        verbose=verbose\n    )\n    \n    # Learning rate reduction callback\n    reduce_lr = ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.5,\n        patience=reduce_lr_patience,\n        min_lr=1e-7,\n        verbose=verbose\n    )\n    \n    # Train model (use the train_ds parameter passed to the function)\n    history = model.fit(\n        train_ds,\n        validation_data=val_ds,\n        epochs=epochs,\n        initial_epoch=initial_epoch,\n        callbacks=[checkpoint_callback, early_stopping, reduce_lr],\n        verbose=verbose\n    )\n    \n    # Load best weights\n    if os.path.exists(checkpoint_path):\n        model.load_weights(checkpoint_path)\n        print(f\" Loaded best weights from {checkpoint_path}\")\n    \n    return model, history\n\nprint(\" Training utility function loaded\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"db6948ea","cell_type":"markdown","source":"## Evaluation Utilities","metadata":{}},{"id":"822bc1cd","cell_type":"code","source":"# EVALUATION FUNCTIONS\n\ndef get_predictions_and_labels(model, dataset, class_names=None):\n    \"\"\"Get predictions and true labels from a dataset.\"\"\"\n    y_true = []\n    y_pred_proba = []\n    \n    # Check if model is InceptionV1Wrapper (PyTorch)\n    if hasattr(model, 'predict') and hasattr(model, 'device'):\n        # PyTorch model\n        for images, labels in dataset:\n            y_true.append(labels.numpy())\n            probs = model.predict(images.numpy())\n            y_pred_proba.append(probs)\n    else:\n        # TensorFlow/Keras model\n        for images, labels in dataset:\n            y_true.append(labels.numpy())\n            probs = model.predict(images, verbose=0)\n            y_pred_proba.append(probs)\n    \n    y_true = np.concatenate(y_true, axis=0)\n    y_pred_proba = np.concatenate(y_pred_proba, axis=0)\n    y_pred = np.argmax(y_pred_proba, axis=1)\n    y_true_classes = np.argmax(y_true, axis=1)\n    \n    return y_true_classes, y_pred, y_pred_proba\n\n\ndef compute_classification_metrics(y_true, y_pred, y_pred_proba, class_names=None):\n    \"\"\"Compute classification metrics.\"\"\"\n    accuracy = accuracy_score(y_true, y_pred)\n    \n    # Per-class and macro metrics\n    precision, recall, f1, support = precision_recall_fscore_support(\n        y_true, y_pred, average=None, zero_division=0\n    )\n    \n    # Macro averages\n    precision_macro = np.mean(precision)\n    recall_macro = np.mean(recall)\n    f1_macro = np.mean(f1)\n    \n    # Classification report\n    if class_names is None:\n        class_names = [f'Class_{i}' for i in range(len(np.unique(y_true)))]\n    \n    report = classification_report(\n        y_true, y_pred, target_names=class_names, output_dict=True, zero_division=0\n    )\n    \n    metrics = {\n        'accuracy': accuracy,\n        'precision_macro': precision_macro,\n        'recall_macro': recall_macro,\n        'f1_macro': f1_macro,\n        'precision_per_class': precision.tolist(),\n        'recall_per_class': recall.tolist(),\n        'f1_per_class': f1.tolist(),\n        'support_per_class': support.tolist(),\n        'classification_report': report\n    }\n    \n    return metrics\n\n\ndef compute_confusion_matrix(y_true, y_pred, class_names=None, normalize=True):\n    \"\"\"Compute confusion matrix.\"\"\"\n    cm = confusion_matrix(y_true, y_pred)\n    \n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        cm = np.nan_to_num(cm)\n    \n    return cm\n\n\ndef compute_roc_curves(y_true, y_pred_proba, class_names=None):\n    \"\"\"Compute ROC curves for each class and micro/macro averages.\"\"\"\n    n_classes = y_pred_proba.shape[1]\n    \n    # Binarize labels for one-vs-rest\n    y_true_bin = label_binarize(y_true, classes=range(n_classes))\n    \n    # Compute ROC for each class\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    \n    for i in range(n_classes):\n        fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_pred_proba[:, i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n    \n    # Micro-average ROC\n    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(\n        y_true_bin.ravel(), y_pred_proba.ravel()\n    )\n    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n    \n    # Macro-average ROC\n    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n    mean_tpr = np.zeros_like(all_fpr)\n    for i in range(n_classes):\n        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n    mean_tpr /= n_classes\n    fpr[\"macro\"] = all_fpr\n    tpr[\"macro\"] = mean_tpr\n    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n    \n    return {\n        'fpr': {str(k): v.tolist() if isinstance(v, np.ndarray) else v for k, v in fpr.items()},\n        'tpr': {str(k): v.tolist() if isinstance(v, np.ndarray) else v for k, v in tpr.items()},\n        'roc_auc': {str(k): v for k, v in roc_auc.items()}\n    }\n\n\ndef evaluate_model(model, test_ds, model_name, class_names=None, save_dir=None):\n    \"\"\"Comprehensive evaluation of a model.\"\"\"\n    if save_dir is None:\n        save_dir = REPORTS_DIR\n    os.makedirs(save_dir, exist_ok=True)\n    \n    print(f\"Evaluating {model_name}...\")\n    \n    # Get predictions\n    y_true, y_pred, y_pred_proba = get_predictions_and_labels(model, test_ds, class_names)\n    \n    # Compute metrics\n    metrics = compute_classification_metrics(y_true, y_pred, y_pred_proba, class_names)\n    cm = compute_confusion_matrix(y_true, y_pred, class_names)\n    roc_data = compute_roc_curves(y_true, y_pred_proba, class_names)\n    \n    # Get model parameter count\n    if hasattr(model, 'count_params'):\n        num_params = model.count_params()\n    elif hasattr(model, 'get_trainable_params_count'):\n        num_params = model.get_trainable_params_count()\n    else:\n        num_params = None\n    \n    results = {\n        'model_name': model_name,\n        'metrics': metrics,\n        'confusion_matrix': cm.tolist(),\n        'roc_data': roc_data,\n        'num_params': num_params\n    }\n    \n    # Save results as JSON\n    results_path = os.path.join(save_dir, f\"{model_name}_results.json\")\n    results_json = json.loads(json.dumps(results, default=str))\n    with open(results_path, 'w') as f:\n        json.dump(results_json, f, indent=2)\n    \n    print(f\" Results saved to {results_path}\")\n    \n    return results\n\nprint(\" Evaluation utility functions loaded\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"67e84121","cell_type":"markdown","source":"## Visualization Utilities","metadata":{}},{"id":"d8e1e3d6","cell_type":"code","source":"# VISUALIZATION FUNCTIONS\n\ndef plot_training_curves(history, model_name, save_path=None, show=True):\n    \"\"\"Plot training and validation loss and accuracy curves.\"\"\"\n    if hasattr(history, 'history'):\n        hist = history.history\n    else:\n        hist = history\n    \n    epochs = range(1, len(hist['loss']) + 1)\n    \n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n    \n    # Loss curve\n    ax1.plot(epochs, hist['loss'], 'b-', label='Training Loss', linewidth=2)\n    ax1.plot(epochs, hist['val_loss'], 'r-', label='Validation Loss', linewidth=2)\n    ax1.set_xlabel('Epoch', fontsize=12)\n    ax1.set_ylabel('Loss', fontsize=12)\n    ax1.set_title(f'{model_name} - Training and Validation Loss', fontsize=14, fontweight='bold')\n    ax1.legend(fontsize=10)\n    ax1.grid(True, alpha=0.3)\n    \n    # Accuracy curve\n    ax2.plot(epochs, hist['accuracy'], 'b-', label='Training Accuracy', linewidth=2)\n    ax2.plot(epochs, hist['val_accuracy'], 'r-', label='Validation Accuracy', linewidth=2)\n    ax2.set_xlabel('Epoch', fontsize=12)\n    ax2.set_ylabel('Accuracy', fontsize=12)\n    ax2.set_title(f'{model_name} - Training and Validation Accuracy', fontsize=14, fontweight='bold')\n    ax2.legend(fontsize=10)\n    ax2.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    \n    if save_path:\n        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n        print(f\" Training curves saved to {save_path}\")\n    \n    if show:\n        plt.show()\n    else:\n        plt.close()\n\n\ndef plot_confusion_matrix(cm, class_names=None, model_name='Model', \n                         normalize=True, save_path=None, show=True):\n    \"\"\"Plot confusion matrix as heatmap.\"\"\"\n    if class_names is None:\n        class_names = [f'Class {i}' for i in range(len(cm))]\n    \n    plt.figure(figsize=(14, 12))\n    \n    cm = np.array(cm)\n    \n    sns.heatmap(cm, annot=True, fmt='.2f' if normalize else 'd', \n                cmap='Blues', xticklabels=class_names, yticklabels=class_names,\n                cbar_kws={'label': 'Normalized Frequency' if normalize else 'Count'})\n    \n    plt.title(f'{model_name} - Confusion Matrix', fontsize=16, fontweight='bold', pad=20)\n    plt.ylabel('True Label', fontsize=12)\n    plt.xlabel('Predicted Label', fontsize=12)\n    plt.xticks(rotation=45, ha='right')\n    plt.yticks(rotation=0)\n    plt.tight_layout()\n    \n    if save_path:\n        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n        print(f\" Confusion matrix saved to {save_path}\")\n    \n    if show:\n        plt.show()\n    else:\n        plt.close()\n\n\ndef plot_roc_curves(roc_data, class_names=None, model_name='Model', \n                   save_path=None, show=True, max_classes_to_plot=10):\n    \"\"\"Plot ROC curves for each class and micro/macro averages.\"\"\"\n    fpr = roc_data['fpr']\n    tpr = roc_data['tpr']\n    roc_auc = roc_data['roc_auc']\n    \n    plt.figure(figsize=(12, 10))\n    \n    # Plot individual class ROC curves\n    class_indices = [int(k) for k in fpr.keys() if k.isdigit()]\n    n_classes = len(class_indices)\n    \n    if n_classes <= max_classes_to_plot:\n        for i in class_indices:\n            if class_names and i < len(class_names):\n                label = f'{class_names[i]} (AUC = {roc_auc[str(i)]:.3f})'\n            else:\n                label = f'Class {i} (AUC = {roc_auc[str(i)]:.3f})'\n            plt.plot(fpr[str(i)], tpr[str(i)], linewidth=1.5, alpha=0.7, label=label)\n    else:\n        sample_indices = np.linspace(0, n_classes-1, max_classes_to_plot, dtype=int)\n        for i in sample_indices:\n            if class_names and i < len(class_names):\n                label = f'{class_names[i]} (AUC = {roc_auc[str(i)]:.3f})'\n            else:\n                label = f'Class {i} (AUC = {roc_auc[str(i)]:.3f})'\n            plt.plot(fpr[str(i)], tpr[str(i)], linewidth=1.5, alpha=0.7, label=label)\n    \n    # Plot micro-average ROC\n    plt.plot(fpr['micro'], tpr['micro'], \n             label=f'Micro-average (AUC = {roc_auc[\"micro\"]:.3f})',\n             linewidth=2, linestyle='--', color='red')\n    \n    # Plot macro-average ROC\n    plt.plot(fpr['macro'], tpr['macro'],\n             label=f'Macro-average (AUC = {roc_auc[\"macro\"]:.3f})',\n             linewidth=2, linestyle='--', color='navy')\n    \n    # Plot diagonal line (random classifier)\n    plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random (AUC = 0.500)')\n    \n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate', fontsize=12)\n    plt.ylabel('True Positive Rate', fontsize=12)\n    plt.title(f'{model_name} - ROC Curves', fontsize=16, fontweight='bold', pad=20)\n    plt.legend(loc='lower right', fontsize=9, ncol=1)\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    \n    if save_path:\n        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n        print(f\" ROC curves saved to {save_path}\")\n    \n    if show:\n        plt.show()\n    else:\n        plt.close()\n\nprint(\" Visualization utility functions loaded\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"26419e8c","cell_type":"markdown","source":"## Grad-CAM Visualization","metadata":{}},{"id":"0fa3273f","cell_type":"code","source":"# Grad-CAM IMPLEMENTATION\n\ndef get_last_conv_layer_name(model):\n    \"\"\"Find the name of the last convolutional layer in a model.\"\"\"\n    for layer in reversed(model.layers):\n        if isinstance(layer, (tf.keras.layers.Conv2D, tf.keras.layers.SeparableConv2D)):\n            return layer.name\n    return None\n\n\ndef make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n    \"\"\"Generate Grad-CAM heatmap for an image.\"\"\"\n    # Create a model that maps input to activations and predictions\n    grad_model = tf.keras.models.Model(\n        [model.inputs], \n        [model.get_layer(last_conv_layer_name).output, model.output]\n    )\n    \n    # Compute gradient\n    with tf.GradientTape() as tape:\n        last_conv_layer_output, preds = grad_model(img_array)\n        if pred_index is None:\n            pred_index = tf.argmax(preds[0])\n        class_channel = preds[:, pred_index]\n    \n    grads = tape.gradient(class_channel, last_conv_layer_output)\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n    \n    # Multiply feature map by importance\n    last_conv_layer_output = last_conv_layer_output[0]\n    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n    heatmap = tf.squeeze(heatmap)\n    \n    # Normalize heatmap\n    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n    \n    return heatmap.numpy()\n\n\ndef overlay_heatmap(img, heatmap, alpha=0.4):\n    \"\"\"Overlay heatmap on original image.\"\"\"\n    heatmap = np.uint8(255 * heatmap)\n    \n    # Use jet colormap\n    jet = plt.cm.get_cmap('jet')\n    jet_colors = jet(np.arange(256))[:, :3]\n    jet_heatmap = jet_colors[heatmap]\n    \n    # Rescale image if needed\n    if img.max() <= 1.0:\n        img = np.uint8(255 * img)\n    else:\n        img = np.uint8(img)\n    \n    # Resize heatmap to match image size\n    if heatmap.shape != img.shape[:2]:\n        jet_heatmap = cv2.resize(jet_heatmap, (img.shape[1], img.shape[0]))\n    \n    # Create overlay\n    jet_heatmap = tf.keras.preprocessing.image.array_to_img(jet_heatmap)\n    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n    jet_heatmap = tf.keras.preprocessing.image.img_to_array(jet_heatmap)\n    \n    superimposed_img = jet_heatmap * alpha + img\n    superimposed_img = np.clip(superimposed_img, 0, 255).astype(np.uint8)\n    \n    return superimposed_img\n\n\ndef visualize_gradcam(model, img_array, class_names=None, last_conv_layer_name=None,\n                     pred_index=None, save_path=None, show=True):\n    \"\"\"Visualize Grad-CAM for a single image.\"\"\"\n    if last_conv_layer_name is None:\n        last_conv_layer_name = get_last_conv_layer_name(model)\n        if last_conv_layer_name is None:\n            raise ValueError(\"Could not find a convolutional layer in the model\")\n    \n    # Get prediction\n    preds = model.predict(img_array, verbose=0)\n    if pred_index is None:\n        pred_index = np.argmax(preds[0])\n    pred_class = class_names[pred_index] if class_names else f'Class {pred_index}'\n    confidence = preds[0][pred_index]\n    \n    # Generate heatmap\n    heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index)\n    \n    # Prepare image for display\n    img = img_array[0]\n    if img.max() <= 1.0:\n        img_display = np.uint8(255 * img)\n    else:\n        img_display = np.uint8(img)\n    \n    # Create overlay\n    superimposed_img = overlay_heatmap(img_display, heatmap)\n    \n    # Plot\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n    \n    axes[0].imshow(img_display)\n    axes[0].set_title('Original Image', fontsize=12, fontweight='bold')\n    axes[0].axis('off')\n    \n    axes[1].imshow(heatmap, cmap='jet')\n    axes[1].set_title('Grad-CAM Heatmap', fontsize=12, fontweight='bold')\n    axes[1].axis('off')\n    \n    axes[2].imshow(superimposed_img)\n    axes[2].set_title(f'Overlay\\nPredicted: {pred_class} ({confidence:.2%})', \n                     fontsize=12, fontweight='bold')\n    axes[2].axis('off')\n    \n    plt.tight_layout()\n    \n    if save_path:\n        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n        print(f\" Grad-CAM visualization saved to {save_path}\")\n    \n    if show:\n        plt.show()\n    else:\n        plt.close()\n\n\ndef visualize_gradcam_samples(model, dataset, num_samples=5, class_names=None,\n                            last_conv_layer_name=None, save_dir=None, \n                            model_name='model'):\n    \"\"\"Visualize Grad-CAM for multiple sample images from a dataset.\"\"\"\n    if save_dir is None:\n        save_dir = REPORTS_DIR\n    os.makedirs(save_dir, exist_ok=True)\n    \n    if last_conv_layer_name is None:\n        last_conv_layer_name = get_last_conv_layer_name(model)\n        if last_conv_layer_name is None:\n            raise ValueError(\"Could not find a convolutional layer in the model\")\n    \n    sample_count = 0\n    for images, labels in dataset:\n        if sample_count >= num_samples:\n            break\n        \n        batch_size = images.shape[0]\n        for i in range(min(batch_size, num_samples - sample_count)):\n            img_array = tf.expand_dims(images[i], 0)\n            true_label = np.argmax(labels[i].numpy())\n            true_class = class_names[true_label] if class_names else f'Class {true_label}'\n            \n            save_path = os.path.join(save_dir, f'{model_name}_gradcam_sample_{sample_count+1}.png')\n            \n            visualize_gradcam(\n                model, img_array, class_names=class_names,\n                last_conv_layer_name=last_conv_layer_name,\n                save_path=save_path, show=False\n            )\n            \n            sample_count += 1\n            if sample_count >= num_samples:\n                break\n    \n    print(f\" Grad-CAM visualizations for {sample_count} samples saved to {save_dir}\")\n\nprint(\" Grad-CAM utility functions loaded\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"5b7d81e7","cell_type":"code","source":"# MODEL COMPARISON\n\ndef compare_models(results_list, class_names=None, save_dir=None):\n    \"\"\"Compare multiple models and generate comparison table.\"\"\"\n    if save_dir is None:\n        save_dir = REPORTS_DIR\n    os.makedirs(save_dir, exist_ok=True)\n    \n    comparison_data = []\n    \n    notes_map = {\n        'vgg19_scratch': 'Trained from scratch with He init, L2 reg, dropout',\n        'resnet50_transfer': 'Transfer learning: frozen base + fine-tuning',\n        'inception_v1': 'GoogLeNet via PyTorch torchhub',\n        'mobilenetv2_transfer': 'MobileNetV2 transfer learning'\n    }\n    \n    for results in results_list:\n        metrics = results['metrics']\n        model_name = results['model_name']\n        comparison_data.append({\n            'Model': model_name,\n            'Accuracy': f\"{metrics['accuracy']:.4f}\",\n            'Precision (Macro)': f\"{metrics['precision_macro']:.4f}\",\n            'Recall (Macro)': f\"{metrics['recall_macro']:.4f}\",\n            'F1 (Macro)': f\"{metrics['f1_macro']:.4f}\",\n            'Params': f\"{results['num_params']:,}\" if results['num_params'] else 'N/A',\n            'Notes': notes_map.get(model_name, '')\n        })\n    \n    df = pd.DataFrame(comparison_data)\n    \n    # Save as CSV\n    csv_path = os.path.join(save_dir, 'model_comparison.csv')\n    df.to_csv(csv_path, index=False)\n    print(f\" Comparison table saved to {csv_path}\")\n    \n    # Save as Markdown\n    md_path = os.path.join(save_dir, 'model_comparison.md')\n    with open(md_path, 'w') as f:\n        f.write(\"# Model Comparison\\n\\n\")\n        f.write(df.to_markdown(index=False))\n        f.write(\"\\n\")\n    \n    print(f\" Comparison markdown saved to {md_path}\")\n    \n    return df\n\nprint(\" Model comparison function loaded\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f61b1c19","cell_type":"markdown","source":"# TRAINING & EVALUATION PIPELINE ","metadata":{}},{"id":"f6a161ab","cell_type":"markdown","source":"## 1. Train VGG19 from Scratch","metadata":{}},{"id":"febaa029","cell_type":"code","source":"print(\"=\" * 80)\nprint(\"Training / Resuming VGG19...\")\nprint(\"=\" * 80)\n\nMODEL_PATH = os.path.join(MODELS_DIR, \"vgg19_scratch.h5\")\n\n#  Load model if checkpoint exists\nif os.path.exists(MODEL_PATH):\n    print(\"Found checkpoint  loading model\")\n    vgg19_model = tf.keras.models.load_model(MODEL_PATH)\n\n    print(\"Recompiling model to ensure eager execution\")\n    vgg19_model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n        loss=\"categorical_crossentropy\",\n        metrics=[\"accuracy\"],\n        run_eagerly=True   #  THIS LINE FIXES YOUR ERROR\n    )\nelse:\n    print(\"No checkpoint found\")\n    print(\"Building VGG19 from scratch\")\n    vgg19_model = build_vgg19_scratch(num_classes=NUM_CLASSES)\n\nvgg19_model.summary()\n\n# Train (continues from last saved state)\nvgg19_model, vgg19_history = train_model(\n    vgg19_model,\n    train_ds_aug,\n    val_ds,\n    name=\"vgg19_scratch\",\n    epochs=MAX_EPOCHS\n)\n\n# Plot training curves\nplot_training_curves(\n    vgg19_history,\n    'VGG19 Scratch',\n    save_path=os.path.join(REPORTS_DIR, 'vgg19_scratch_training_curves.png')\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"ec10838f","cell_type":"markdown","source":"## 2. Train ResNet50 Transfer Learning","metadata":{}},{"id":"64202406","cell_type":"code","source":"# Build ResNet50 with transfer learning\nprint(\"=\" * 80)\nprint(\"Training ResNet50 with transfer learning...\")\nprint(\"=\" * 80)\n\n# Phase 1: Train classifier head only\nresnet50_model, resnet50_base = build_resnet50_transfer(num_classes=NUM_CLASSES)\nresnet50_model.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nprint(\"Phase 1: Training classifier head (base frozen)...\")\nresnet50_model, resnet50_history1 = train_model(\n    resnet50_model, train_ds_aug, val_ds, 'resnet50_transfer_head', \n    epochs=MAX_EPOCHS,  # Use global MAX_EPOCHS\n    early_stopping_patience=3,  # Faster early stopping\n    reduce_lr_patience=2\n)\n\n# Phase 2: Fine-tuning\nprint(\"Phase 2: Fine-tuning (unfreezing last 40 layers)...\")\nresnet50_model = unfreeze_resnet_for_finetuning(resnet50_model, resnet50_base, num_layers_to_unfreeze=40)\n\nresnet50_model, resnet50_history2 = train_model(\n    resnet50_model, train_ds_aug, val_ds, 'resnet50_transfer', \n    epochs=MAX_EPOCHS,  # Use global MAX_EPOCHS initial_epoch=MAX_EPOCHS,\n    early_stopping_patience=3,  # Faster early stopping\n    reduce_lr_patience=2\n)\n\n# Combine histories\ncombined_history = {\n    'loss': resnet50_history1.history['loss'] + resnet50_history2.history['loss'],\n    'accuracy': resnet50_history1.history['accuracy'] + resnet50_history2.history['accuracy'],\n    'val_loss': resnet50_history1.history['val_loss'] + resnet50_history2.history['val_loss'],\n    'val_accuracy': resnet50_history1.history['val_accuracy'] + resnet50_history2.history['val_accuracy']\n}\n\nclass CombinedHistory:\n    def __init__(self, history_dict):\n        self.history = history_dict\n\nresnet50_history = CombinedHistory(combined_history)\n\n# Plot training curves\nplot_training_curves(\n    resnet50_history, 'ResNet50 Transfer',\n    save_path=os.path.join(REPORTS_DIR, 'resnet50_transfer_training_curves.png')\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e4516b2b","cell_type":"markdown","source":"## 3. Train MobileNetV2 Transfer Learning\n","metadata":{}},{"id":"1db0faf1","cell_type":"code","source":"# Build and train MobileNetV2 with transfer learning\nprint(\"=\" * 80)\nprint(\"Training MobileNetV2 with transfer learning...\")\nprint(\"=\" * 80)\n\nmobilenet_model, mobilenet_base = build_mobilenetv2_transfer(\n    num_classes=NUM_CLASSES,\n    weights='imagenet' if USE_PRETRAINED_WEIGHTS else None\n)\nmobilenet_model.summary()\n\n# Train model with optimized callbacks for faster training\nmobilenet_model, mobilenet_history = train_model(\n    mobilenet_model, train_ds_aug, val_ds, 'mobilenetv2_transfer', \n    epochs=MAX_EPOCHS,\n    early_stopping_patience=3,  # Faster early stopping\n    reduce_lr_patience=2\n)\n\n# Plot training curves\nplot_training_curves(\n    mobilenet_history, 'MobileNetV2 Transfer',\n    save_path=os.path.join(REPORTS_DIR, 'mobilenetv2_transfer_training_curves.png')\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"dc890c16","cell_type":"markdown","source":"## 4. Train Inception V1 (GoogLeNet) - PyTorch","metadata":{}},{"id":"1269705d","cell_type":"code","source":"# Train Inception V1 (GoogLeNet) using PyTorch\nif TORCH_AVAILABLE:\n    print(\"=\" * 80)\n    print(\"Training Inception V1 (GoogLeNet) with PyTorch...\")\n    print(\"=\" * 80)\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Training on device: {device}\")\n    \n    # Build model\n    inception_model = InceptionV1Wrapper(num_classes=NUM_CLASSES, pretrained=USE_PRETRAINED_WEIGHTS)\n    inception_model.model.train()\n    \n    # Convert TensorFlow dataset to streaming PyTorch loader (no full concat to avoid OOM)\n    class TfDatasetIterable(torch.utils.data.IterableDataset):\n        def __init__(self, dataset):\n            self.dataset = dataset\n        def __iter__(self):\n            for images, labels in self.dataset:\n                images_torch = torch.from_numpy(images.numpy()).permute(0, 3, 1, 2).float()\n                labels_torch = torch.from_numpy(labels.numpy()).float()\n                yield images_torch, labels_torch\n    \n    train_loader = DataLoader(TfDatasetIterable(train_ds_aug), batch_size=None)\n    val_loader = DataLoader(TfDatasetIterable(val_ds), batch_size=None)\n    \n    def extract_logits(outputs):\n        if hasattr(outputs, 'logits'):\n            return outputs.logits\n        if isinstance(outputs, tuple):\n            return outputs[0]\n        return outputs\n    \n    # Optimizer and loss\n    optimizer = optim.Adam(inception_model.model.parameters(), lr=1e-3)\n    criterion = nn.CrossEntropyLoss()\n    \n    # Training history\n    inception_history = {\n        'loss': [],\n        'accuracy': [],\n        'val_loss': [],\n        'val_accuracy': []\n    }\n    \n    best_val_loss = float('inf')\n    patience = 3  # Reduced from 5 for faster training\n    patience_counter = 0\n    epochs = MAX_EPOCHS  # Use global MAX_EPOCHS\n    \n    for epoch in range(epochs):\n        # Training phase\n        inception_model.model.train()\n        train_loss = 0.0\n        train_correct = 0\n        train_total = 0\n        \n        train_batches = 0\n        for images, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs} [Train]'):\n            train_batches += 1\n            images = images.to(device)\n            labels = labels.to(device)\n            labels_classes = labels.argmax(dim=1)\n            \n            optimizer.zero_grad()\n            outputs = inception_model.model(images)\n            logits = extract_logits(outputs)\n            loss = criterion(logits, labels_classes)\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item()\n            _, predicted = torch.max(logits.data, 1)\n            train_total += labels_classes.size(0)\n            train_correct += (predicted == labels_classes).sum().item()\n        \n        train_loss /= max(1, train_batches)\n        train_acc = train_correct / max(1, train_total)\n        \n        # Validation phase\n        inception_model.model.eval()\n        val_loss = 0.0\n        val_correct = 0\n        val_total = 0\n        \n        val_batches = 0\n        with torch.no_grad():\n            for images, labels in tqdm(val_loader, desc=f'Epoch {epoch+1}/{epochs} [Val]'):\n                val_batches += 1\n                images = images.to(device)\n                labels = labels.to(device)\n                labels_classes = labels.argmax(dim=1)\n                \n                outputs = inception_model.model(images)\n                logits = extract_logits(outputs)\n                loss = criterion(logits, labels_classes)\n                \n                val_loss += loss.item()\n                _, predicted = torch.max(logits.data, 1)\n                val_total += labels_classes.size(0)\n                val_correct += (predicted == labels_classes).sum().item()\n        \n        val_loss /= max(1, val_batches)\n        val_acc = val_correct / max(1, val_total)\n        \n        # Update history\n        inception_history['loss'].append(train_loss)\n        inception_history['accuracy'].append(train_acc)\n        inception_history['val_loss'].append(val_loss)\n        inception_history['val_accuracy'].append(val_acc)\n        \n        print(f'Epoch {epoch+1}/{epochs} - '\n              f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, '\n              f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n        \n        # Early stopping\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            patience_counter = 0\n            inception_model.save(os.path.join(MODELS_DIR, 'inception_v1.pth'))\n            print(f'   Saved best model (val_loss: {val_loss:.4f})')\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(f'Early stopping at epoch {epoch+1}')\n                break\n    \n    # Load best model\n    inception_model.load(os.path.join(MODELS_DIR, 'inception_v1.pth'))\n    inception_model.model.eval()\n    \n    # Plot training curves\n    plot_training_curves(\n        inception_history, 'Inception V1',\n        save_path=os.path.join(REPORTS_DIR, 'inception_v1_training_curves.png')\n    )\n    \n    print(\"Inception V1 training complete\")\nelse:\n    print(\" Skipping Inception V1 (PyTorch not available)\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"576abff8","cell_type":"code","source":"# Evaluate all trained models\nprint(\"=\" * 80)\nprint(\"Evaluating all models...\")\nprint(\"=\" * 80)\n\nall_results = []\n\n# Evaluate VGG19\nprint(\"\\n1. Evaluating VGG19...\")\nvgg19_results = evaluate_model(vgg19_model, test_ds, 'vgg19_scratch', class_names=CLASS_NAMES)\nvgg19_results['history'] = vgg19_history\nall_results.append(vgg19_results)\n\n# Plot evaluations for VGG19\nplot_confusion_matrix(\n    vgg19_results['confusion_matrix'],\n    class_names=CLASS_NAMES,\n    model_name='VGG19 Scratch',\n    save_path=os.path.join(REPORTS_DIR, 'vgg19_scratch_confusion_matrix.png'),\n    show=False\n)\n\nplot_roc_curves(\n    vgg19_results['roc_data'],\n    class_names=CLASS_NAMES,\n    model_name='VGG19 Scratch',\n    save_path=os.path.join(REPORTS_DIR, 'vgg19_scratch_roc_curves.png'),\n    show=False\n)\n\n# Grad-CAM for VGG19\ntry:\n    visualize_gradcam_samples(\n        vgg19_model, test_ds, num_samples=5, class_names=CLASS_NAMES,\n        model_name='vgg19_scratch'\n    )\nexcept Exception as e:\n    print(f\"Grad-CAM failed for VGG19: {e}\")\n\nprint(\" VGG19 evaluation complete\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a388a785","cell_type":"code","source":"# Evaluate ResNet50\nprint(\"\\n2. Evaluating ResNet50...\")\nresnet50_results = evaluate_model(resnet50_model, test_ds, 'resnet50_transfer', class_names=CLASS_NAMES)\nresnet50_results['history'] = resnet50_history\nall_results.append(resnet50_results)\n\n# Plot evaluations for ResNet50\nplot_confusion_matrix(\n    resnet50_results['confusion_matrix'],\n    class_names=CLASS_NAMES,\n    model_name='ResNet50 Transfer',\n    save_path=os.path.join(REPORTS_DIR, 'resnet50_transfer_confusion_matrix.png'),\n    show=False\n)\n\nplot_roc_curves(\n    resnet50_results['roc_data'],\n    class_names=CLASS_NAMES,\n    model_name='ResNet50 Transfer',\n    save_path=os.path.join(REPORTS_DIR, 'resnet50_transfer_roc_curves.png'),\n    show=False\n)\n\n# Grad-CAM for ResNet50\ntry:\n    visualize_gradcam_samples(\n        resnet50_model, test_ds, num_samples=5, class_names=CLASS_NAMES,\n        model_name='resnet50_transfer'\n    )\nexcept Exception as e:\n    print(f\"Grad-CAM failed for ResNet50: {e}\")\n\nprint(\" ResNet50 evaluation complete\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"5fa7b62c","cell_type":"code","source":"# Evaluate MobileNetV2\nprint(\"\\n3. Evaluating MobileNetV2...\")\nmobilenet_results = evaluate_model(mobilenet_model, test_ds, 'mobilenetv2_transfer', class_names=CLASS_NAMES)\nmobilenet_results['history'] = mobilenet_history\nall_results.append(mobilenet_results)\n\n# Plot evaluations for MobileNetV2\nplot_confusion_matrix(\n    mobilenet_results['confusion_matrix'],\n    class_names=CLASS_NAMES,\n    model_name='MobileNetV2 Transfer',\n    save_path=os.path.join(REPORTS_DIR, 'mobilenetv2_transfer_confusion_matrix.png'),\n    show=False\n)\n\nplot_roc_curves(\n    mobilenet_results['roc_data'],\n    class_names=CLASS_NAMES,\n    model_name='MobileNetV2 Transfer',\n    save_path=os.path.join(REPORTS_DIR, 'mobilenetv2_transfer_roc_curves.png'),\n    show=False\n)\n\n# Grad-CAM for MobileNetV2\ntry:\n    visualize_gradcam_samples(\n        mobilenet_model, test_ds, num_samples=5, class_names=CLASS_NAMES,\n        model_name='mobilenetv2_transfer'\n    )\nexcept Exception as e:\n    print(f\"Grad-CAM failed for MobileNetV2: {e}\")\n\nprint(\" MobileNetV2 evaluation complete\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"9426b945","cell_type":"code","source":"# Evaluate Inception V1 (if trained)\nif TORCH_AVAILABLE and 'inception_model' in locals():\n    print(\"\\n4. Evaluating Inception V1...\")\n    inception_results = evaluate_model(inception_model, test_ds, 'inception_v1', class_names=CLASS_NAMES)\n    inception_results['history'] = inception_history\n    all_results.append(inception_results)\n    \n    # Plot evaluations for Inception V1\n    plot_confusion_matrix(\n        inception_results['confusion_matrix'],\n        class_names=CLASS_NAMES,\n        model_name='Inception V1',\n        save_path=os.path.join(REPORTS_DIR, 'inception_v1_confusion_matrix.png'),\n        show=False\n    )\n    \n    plot_roc_curves(\n        inception_results['roc_data'],\n        class_names=CLASS_NAMES,\n        model_name='Inception V1',\n        save_path=os.path.join(REPORTS_DIR, 'inception_v1_roc_curves.png'),\n        show=False\n    )\n    \n    print(\" Inception V1 evaluation complete\")\nelse:\n    print(\" Skipping Inception V1 evaluation (not trained)\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"909f9315","cell_type":"markdown","source":"## Model Comparison\n","metadata":{}},{"id":"09efa073","cell_type":"code","source":"# Generate model comparison table\nprint(\"=\" * 80)\nprint(\"Generating model comparison...\")\nprint(\"=\" * 80)\n\ncomparison_df = compare_models(all_results, class_names=CLASS_NAMES)\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"MODEL COMPARISON TABLE\")\nprint(\"=\" * 80)\nprint(comparison_df.to_string(index=False))\nprint(\"=\" * 80)\n\n# Display the comparison table\ncomparison_df\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}